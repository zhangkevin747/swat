{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ff3f583",
   "metadata": {},
   "source": [
    "# Guard Model Evaluation Pipeline\n",
    "\n",
    "Batch-generates responses to adversarial prompts and classifies them with Llama Guard 3.\n",
    "\n",
    "Each `# %%` block is a notebook cell. Also runs as a standalone script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc4ca0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Config\n",
    "\n",
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Configuration ---\n",
    "GENERATOR_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "GUARD_MODEL = \"meta-llama/Llama-Guard-3-1B\"\n",
    "QUESTIONS_FILE = \"questions.txt\"        # one question per line\n",
    "N_RESPONSES = 10                        # responses per question\n",
    "GEN_BATCH_SIZE = 8                      # generation batch size\n",
    "GUARD_BATCH_SIZE = 8                    # guard classification batch size\n",
    "MAX_NEW_TOKENS = 256\n",
    "TEMPERATURE = 0.7\n",
    "OUTPUT_FILE = \"guard_results.json\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE = torch.float16 if DEVICE == \"cuda\" else torch.float32\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56601893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading generator: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d5b5388b6984755a67ccd84af2e408d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96fb50de65b04055ad320ec1de0b0fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48db0877db2b42c9b2d0143bc5e09101",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8385b49e15df43d88fef52a39a79bcf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6108c2913e143b89f219158fdf6d3d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e4412f2bcef45e9870cce1e25c231c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e00e7fc5bca4d6b8669928f93455a6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "778c581aa8dd4c258310394007feb8a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:01<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 2: Load generator model\n",
    "\n",
    "print(f\"Loading generator: {GENERATOR_MODEL}\")\n",
    "gen_tokenizer = AutoTokenizer.from_pretrained(GENERATOR_MODEL)\n",
    "gen_model = AutoModelForCausalLM.from_pretrained(\n",
    "    GENERATOR_MODEL,\n",
    "    torch_dtype=DTYPE,\n",
    "    device_map=\"auto\" if DEVICE == \"cuda\" else None,\n",
    ")\n",
    "\n",
    "# Left-pad for batched decoder-only generation\n",
    "gen_tokenizer.padding_side = \"left\"\n",
    "if gen_tokenizer.pad_token is None:\n",
    "    gen_tokenizer.pad_token = gen_tokenizer.eos_token\n",
    "    gen_model.config.pad_token_id = gen_tokenizer.eos_token_id\n",
    "\n",
    "print(\"Generator loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa06530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load questions & build prompt list\n",
    "questions = [q.strip() for q in Path(QUESTIONS_FILE).read_text().strip().splitlines() if q.strip()]\n",
    "print(f\"Loaded {len(questions)} questions, generating {N_RESPONSES} responses each → {len(questions) * N_RESPONSES} total\")\n",
    "\n",
    "# Repeat each formatted prompt N times\n",
    "prompts = []        # formatted chat strings\n",
    "q_indices = []      # maps each prompt back to its question index\n",
    "\n",
    "for qi, question in enumerate(questions):\n",
    "    chat = [{\"role\": \"user\", \"content\": question}]\n",
    "    formatted = gen_tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "    for _ in range(N_RESPONSES):\n",
    "        prompts.append(formatted)\n",
    "        q_indices.append(qi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d4a3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Batch-generate responses\n",
    "responses = []\n",
    "\n",
    "for start in tqdm(range(0, len(prompts), GEN_BATCH_SIZE), desc=\"Generating\"):\n",
    "    batch = prompts[start : start + GEN_BATCH_SIZE]\n",
    "    inputs = gen_tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to(gen_model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = gen_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            do_sample=True,\n",
    "            temperature=TEMPERATURE,\n",
    "            pad_token_id=gen_tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    for i, ids in enumerate(out):\n",
    "        prompt_len = inputs[\"input_ids\"].shape[1]          # same for whole batch (left-padded)\n",
    "        text = gen_tokenizer.decode(ids[prompt_len:], skip_special_tokens=True)\n",
    "        responses.append(text)\n",
    "\n",
    "print(f\"Generated {len(responses)} responses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5da3f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Free generator, load guard model\n",
    "\n",
    "del gen_model, gen_tokenizer\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"Loading guard: {GUARD_MODEL}\")\n",
    "guard_tokenizer = AutoTokenizer.from_pretrained(GUARD_MODEL)\n",
    "guard_model = AutoModelForCausalLM.from_pretrained(\n",
    "    GUARD_MODEL,\n",
    "    torch_dtype=DTYPE,\n",
    "    device_map=\"auto\" if DEVICE == \"cuda\" else None,\n",
    ")\n",
    "\n",
    "guard_tokenizer.padding_side = \"left\"\n",
    "if guard_tokenizer.pad_token is None:\n",
    "    guard_tokenizer.pad_token = guard_tokenizer.eos_token\n",
    "\n",
    "print(\"Guard model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971199c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Format conversations for guard\n",
    "guard_texts = []\n",
    "for i, response in enumerate(responses):\n",
    "    convo = [\n",
    "        {\"role\": \"user\", \"content\": questions[q_indices[i]]},\n",
    "        {\"role\": \"assistant\", \"content\": response},\n",
    "    ]\n",
    "    guard_texts.append(guard_tokenizer.apply_chat_template(convo, tokenize=False))\n",
    "\n",
    "print(f\"Prepared {len(guard_texts)} conversations for classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1d380e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Batch-classify with guard model\n",
    "\n",
    "results = []\n",
    "\n",
    "for start in tqdm(range(0, len(guard_texts), GUARD_BATCH_SIZE), desc=\"Classifying\"):\n",
    "    batch = guard_texts[start : start + GUARD_BATCH_SIZE]\n",
    "    inputs = guard_tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to(guard_model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = guard_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            pad_token_id=guard_tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    prompt_len = inputs[\"input_ids\"].shape[1]\n",
    "    for i, ids in enumerate(out):\n",
    "        guard_output = guard_tokenizer.decode(ids[prompt_len:], skip_special_tokens=True).strip()\n",
    "        idx = start + i\n",
    "        results.append({\n",
    "            \"question_idx\": q_indices[idx],\n",
    "            \"question\": questions[q_indices[idx]],\n",
    "            \"response\": responses[idx],\n",
    "            \"guard_output\": guard_output,\n",
    "            \"is_unsafe\": \"unsafe\" in guard_output.lower(),\n",
    "        })\n",
    "\n",
    "print(f\"Classified {len(results)} responses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be139d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Results summary\n",
    "\n",
    "total = len(results)\n",
    "unsafe = sum(r[\"is_unsafe\"] for r in results)\n",
    "safe = total - unsafe\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"  OVERALL: {unsafe}/{total} unsafe ({100*unsafe/total:.1f}%)\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"\\n  PER-QUESTION BREAKDOWN\")\n",
    "print(f\"  {'-'*56}\")\n",
    "for qi, q in enumerate(questions):\n",
    "    q_res = [r for r in results if r[\"question_idx\"] == qi]\n",
    "    q_unsafe = sum(r[\"is_unsafe\"] for r in q_res)\n",
    "    pct = 100 * q_unsafe / len(q_res)\n",
    "    bar = \"█\" * int(pct // 10) + \"░\" * (10 - int(pct // 10))\n",
    "    label = q[:60] + (\"...\" if len(q) > 60 else \"\")\n",
    "    print(f\"  Q{qi+1:>3}: {q_unsafe:>2}/{len(q_res)} {bar} {pct:5.1f}%  {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fd4c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Save detailed results\n",
    "payload = {\n",
    "    \"config\": {\n",
    "        \"generator\": GENERATOR_MODEL,\n",
    "        \"guard\": GUARD_MODEL,\n",
    "        \"n_responses\": N_RESPONSES,\n",
    "        \"temperature\": TEMPERATURE,\n",
    "        \"max_new_tokens\": MAX_NEW_TOKENS,\n",
    "        \"num_questions\": len(questions),\n",
    "    },\n",
    "    \"summary\": {\n",
    "        \"total\": total,\n",
    "        \"unsafe\": unsafe,\n",
    "        \"safe\": safe,\n",
    "        \"unsafe_pct\": round(100 * unsafe / total, 2),\n",
    "    },\n",
    "    \"per_question\": [\n",
    "        {\n",
    "            \"question\": q,\n",
    "            \"unsafe\": sum(r[\"is_unsafe\"] for r in results if r[\"question_idx\"] == qi),\n",
    "            \"total\": N_RESPONSES,\n",
    "            \"unsafe_pct\": round(100 * sum(r[\"is_unsafe\"] for r in results if r[\"question_idx\"] == qi) / N_RESPONSES, 2),\n",
    "        }\n",
    "        for qi, q in enumerate(questions)\n",
    "    ],\n",
    "    \"details\": results,\n",
    "}\n",
    "\n",
    "Path(OUTPUT_FILE).write_text(json.dumps(payload, indent=2))\n",
    "print(f\"\\nDetailed results saved to {OUTPUT_FILE}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
